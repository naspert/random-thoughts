{
  
    
        "post0": {
            "title": "Pre-processing wikipedia dumps with dask (part 1)",
            "content": "Introduction . In addition to being a great source of information, Wikipedia is also a model in openness and access to its data. You can access the whole data from the dumps page and download most of the data powering Wikipedia. . I will focus here on how to process the SQL dumps and transform them to be ingested by, for instance, a Neo4j graph database. This can be done by other means, for instance using the sparkwiki tool (disclaimer: mostly written by me). Sparkwiki is great, however it requires a spark instance/cluster to work, which is not always easy to set up, even with Bigtop or elasticluster. Nowadays, the research world runs a lot of Python, and dask seems (claims ?) to be the Python-based Spark equivalent, so making an equivalent of sparkwiki in python is a good excuse to see if dask is fit for the job. . Requirements . In order to run the experiments below, you need . a good internet connection (the dumps amount for ca. 8 GB of data you need to download) | a computer with a decent amount of RAM and CPUs (I ran them on a 2x20-cores system with 128 GB of RAM, but never used it fully) or a suitable cluster if you have one at hand, and preferrably SSD local storage. I will asssume a Linux-based OS, though it should work on MacOS or even Windows. | a suitable conda/pip environment with the necessary packages installed (TODO) | . SQL dumps . In this example, I will show how to process the english wikipedia database dump, the adaptation to other languages should be fairly straightforward. You can find the index of all dumps on this page, the english language SQL dumps are in the enwiki subfolder. In order to achieve speedy downloads, you can use of the mirrors close to you (I am in Switzerland, the mirror from Umeå university is the fastest one for me, YMMV). The latest backup available (when I wrote this) is from Nov. 20th 2020, I will be using this one in my examples. . Files needed . You will need 3 files : . enwiki-20201120-page.sql.gz which contains page data (title, namespace, etc.) | enwiki-20201120-pagelinks.sql.gz which contains all links between pages within the English wikipedia edition (links across languages are stored in a different table) | enwiki-20201120-redirect.sql.gz which contains redirection information betweem pages (e.g when a page is renamed) | . You can find more detailed information about each table on the database layout page. . data_path = &#39;/data/wikipedia/20201120&#39; . Parallelism - preliminary step . Sparkwiki was able to process the compressed dumps in parallel after converting them to bz2 archives. After experimenting with dask, it turns out this simple step is not sufficient to achieve parallelism. However, dask supports reading multiple files in parallel, so an extra step is needed to convert the single-file SQL dump into multiple files. Those files can be used to re-create the database powering Wikipedia, and is just a big text file containing INSERT statements (quite a lot of them actually). . The handy Linux command split allows cutting the files into smaller chunks, without breaking SQL statements which need to be parsed later on: . zcat enwiki-20201101-redirect.sql.gz | split -l 100 --filter &#39;bzip2 &gt; $FILE.bz2&#39; - split-redirect- . This command will cut the redirect SQL dump into 100-lines bzip2-compressed (thanks to the filter parameter) chunks with a split-redirect- prefix. Check the man page of the command for more details. . We can now split the two other files. The number of lines in each chunk is hand-adjusted to have a good compromise between number of files and file size : . zcat enwiki-20201101-page.sql.gz | split -l 150 --filter &#39;bzip2 &gt; $FILE.bz2&#39; - split-page- zcat enwiki-20201101-pagelinks.sql.gz | split -l 400 --filter &#39;bzip2 &gt; $FILE.bz2&#39; - split-pagelinks- . This can take a few hours (esp. the splitting of the pagelinks file), so if you want to skip this part you can use the splits I uploaded on our S3-compatible storage hosted on SwitchEngines. . Processing SQL dumps . We are now ready to actually perform the processing. . Imports . import pandas as pd import dask import dask.bag as db import dask.dataframe as ddf import re import os . Starting the dask (local) cluster . Before running commands, we need to set up a few things for processing. NB: This is NOT the only way to do it, you can read about setting up the scheduler on a single machine, or on other types of environments such as clouds, Kubernetes, etc. One important thing to set up when running on a single computer is to have a local temporary directory: at some point when processing, dask will write data to disk. It is fairly common to have NFS-mounted directories for instance, and it is crucial for good performance to make sure data does not need to go through a network. . dask.config.set({&#39;temporary_directory&#39;: &#39;/tmp&#39;}) # make sure temp dir is local and has sufficient space. Adjust to your resources/needs ! . from dask.distributed import LocalCluster, Client . We can now start our local cluster. This will work for me, make sure you adapt to your local resources ! It is important to keep $ text{n_workers} times text{memory_limit}$ under the physical memory available (or a lower value if the computer is shared with other users and you want to be nice to others). . cluster = LocalCluster(n_workers=6, threads_per_worker=4, memory_limit=24e9) client = Client(cluster) . Helpers functions . Let us now set up a few routines and regexps (check sparkwiki for details) . re_dict = {&quot;page&quot;: &quot; (( d+),( d+),&#39;(.*?)&#39;,&#39;(.*?)&#39;,([01]),([01]),([ d .]+?),&#39;( d{14})&#39;,(.*?),( d+),( d+),(.*?),(.*?) )&quot;, &quot;redirect&quot;:&quot; (( d+),( d+),&#39;(.*?)&#39;,(.*?),(.*?) )&quot;, &quot;pagelinks&quot;:&quot; (( d+),( d+),&#39;(.*?)&#39;,( d+) )&quot;} . def filter_line(line, dump_type): return line.startswith(&#39;INSERT INTO `{}` VALUES&#39;.format(dump_type)) . def split_line(line): return line.split(&quot; VALUES &quot;)[1].strip() . def get_redirect(rec): return {&#39;from&#39;:int(rec[0]), &#39;target_ns&#39;:int(rec[1]), &#39;title&#39;:rec[2], &#39;inter_wiki&#39;:rec[3], &#39;fragment&#39;:rec[4]} . def get_page(rec): # case class WikipediaSimplePage(id:Int, title:String, isRedirect:Boolean, isNew: Boolean) return {&#39;id&#39;: int(rec[0]), &#39;namespace&#39;:int(rec[1]), &#39;title&#39;: rec[2], &#39;is_redirect&#39;:int(rec[4])==1, &#39;is_new&#39;:int(rec[5])==1} . def get_pagelinks(rec): # case class WikipediaPagelinks(from:Int, namespace:Int, title:String, fromNamespace:Int) return {&#39;from&#39;: int(rec[0]), &#39;namespace&#39;:int(rec[1]), &#39;title&#39;:rec[2], &#39;from_namespace&#39;:int(rec[3])} . Processing redirects . It may seem surprising to start with this particular table. As it is the smallest one, it is often more convenient to start and experiment processing with this one. If you make a mistake, you don&#39;t have to wait for a long time before the crash. . Read the local files into a dask bag . A dask bag is the equivalent of a Spark RDD. It supports simple operations such as filter, map, etc. . redirects_bag = db.read_text(os.path.join(data_path, &#39;splits/split-redirect-*.bz2&#39;) . Alternative: read the files from S3 bucket . It is not hosted on Amazon but on the Swiss Universities cloud SwitchEngines, hence the custom options needed below . storage_options={&#39;anon&#39;:True, &#39;client_kwargs&#39;:{&#39;endpoint_url&#39;:&#39;https://os.unil.cloud.switch.ch&#39;}} . redirects_bag = db.read_text(&#39;s3://lts2-wikipedia/enwiki/20201120/splits/split-redirect-*.bz2&#39;, storage_options=storage_options) . Tansform each record into a redirect dictionary . We can chain conveniently the operators to . filter out non INSERT statements using filter_line | split all VALUES from the INSERT using split_line and the appropriate regexp | convert to a dictionary using get_redirect | . Sparkwiki does the same operations, check the WikipediaElementParser.scala file for more details. . redirects = redirects_bag.filter(lambda x: filter_line(x, &#39;redirect&#39;)) .map(split_line) .map(lambda x:re.findall(re_dict[&#39;redirect&#39;], x)).flatten() .map(get_redirect) . Finally, the collection of small redirect dictionaries is converted to a dask DataFrame (similar to a pandas DataFrame) . redirects_df = redirects.to_dataframe() . Filter out all redirects that do not concern namespace 0 (= articles), cf. Wikipedia namespaces . redirects_df_filt = redirects_df[redirects_df[&#39;target_ns&#39;]==0] . At this point, nothing is computed yet. Setting the index and saving the resulting DataFrame to a parquet file will trigger all computations. You can monitor what is happening under the hood by opening a connection to the dask scheduler web interface which should run on port 8787. It takes less than a minute on the system I have at hand. . redirects_df_filt.set_index(&#39;from&#39;).to_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;redirect.parquet&#39;), compression=&#39;gzip&#39;, engine=&#39;fastparquet&#39;) . You can call the usual pandas DataFrame methos such as head, tailetc. However, calling them directly on redirects_df_filtwill trigger a computation. If you want to avoid this, you can reload it from disk : . redirects_df_reloaded = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;redirect.parquet&#39;)) redirects_df_reloaded.head() . target_ns title inter_wiki fragment . from . 10 0 | Computer_accessibility | &#39;&#39; | &#39;&#39; | . 13 0 | History_of_Afghanistan | &#39;&#39; | &#39;&#39; | . 14 0 | Geography_of_Afghanistan | &#39;&#39; | &#39;&#39; | . 15 0 | Demographics_of_Afghanistan | &#39;&#39; | &#39;&#39; | . 18 0 | Communications_in_Afghanistan | &#39;&#39; | &#39;&#39; | . Processing pages . Almost identical to redirects, takes longer though . Read splits from local filesystem . pages_bag = db.read_text(os.path.join(data_path, &#39;splits/split-page-*.bz2&#39;) . Alternative: read from S3 storage . pages_bag = db.read_text(&#39;s3://lts2-wikipedia/enwiki/20201120/splits/split-page-*.bz2&#39;, storage_options=storage_options) . Filter records and create dataframe . pages = pages_bag.filter(lambda x: filter_line(x, &#39;page&#39;)) .map(split_line) .map(lambda x:re.findall(re_dict[&#39;page&#39;], x)) .flatten().map(get_page) . pages_df = pages.to_dataframe() . Keep only namespace 0 pages . pages_filt_df = pages_df[pages_df[&#39;namespace&#39;]==0] . Trigger computation and save result (runs for a few minutes on my system) . pages_filt_df.set_index(&#39;id&#39;).to_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pages.parquet&#39;), compression=&#39;gzip&#39;, engine=&#39;fastparquet&#39;) . Check what we now have in the dataframe : . pages_df_reloaded = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pages.parquet&#39;)) pages_df_reloaded.head(20) . namespace title is_redirect is_new . id . 10 0 | AccessibleComputing | True | False | . 12 0 | Anarchism | False | False | . 13 0 | AfghanistanHistory | True | False | . 14 0 | AfghanistanGeography | True | False | . 15 0 | AfghanistanPeople | True | False | . 18 0 | AfghanistanCommunications | True | False | . 19 0 | AfghanistanTransportations | True | False | . 20 0 | AfghanistanMilitary | True | False | . 21 0 | AfghanistanTransnationalIssues | True | False | . 23 0 | AssistiveTechnology | True | False | . 24 0 | AmoeboidTaxa | True | False | . 25 0 | Autism | False | False | . 27 0 | AlbaniaHistory | True | False | . 29 0 | AlbaniaPeople | True | False | . 30 0 | AsWeMayThink | True | False | . 35 0 | AlbaniaGovernment | True | False | . 36 0 | AlbaniaEconomy | True | False | . 39 0 | Albedo | False | False | . 40 0 | AfroAsiaticLanguages | True | False | . 42 0 | ArtificalLanguages | True | False | . Processing pagelinks . NB: this is the biggest part to process. It seems some entries can trigger utf-8 decoding errors, hence the errors=&#39;backslashreplace&#39; addition whem reading. No index will be created to speed up computation . pagelinks_bag = db.read_text(os.path.join(data_path, &#39;splits/split-pagelinks-*.bz2&#39;), errors=&#39;backslashreplace&#39;) . Or S3-hosted splits : . pageslinks_bag = db.read_text(&#39;s3://lts2-wikipedia/enwiki/20201120/splits/split-pagelinks-*.bz2&#39;, storage_options=storage_options, errors=&#39;backslashreplace&#39;) . get pagelinks dictionaries . pagelinks = pagelinks_bag.filter(lambda x: filter_line(x, &#39;pagelinks&#39;)) .map(split_line) .map(lambda x:re.findall(re_dict[&#39;pagelinks&#39;], x)) .flatten().map(get_pagelinks) . pagelinks_df = pagelinks.to_dataframe() . Only keep links between articles (namespace == 0), discard all the others . pagelinks_filt_df = pagelinks_df[(pagelinks_df[&#39;namespace&#39;] == 0) &amp; (pagelinks_df[&#39;from_namespace&#39;] == 0)] . pagelinks_filt_df.to_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks.parquet&#39;), compression=&#39;gzip&#39;, engine=&#39;fastparquet&#39;) . pagelinks_reloaded = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks.parquet&#39;)) pagelinks_reloaded.head(20) . from namespace title from_namespace . 0 4748 | 0 | ! | 0 | . 1 9773 | 0 | ! | 0 | . 2 15154 | 0 | ! | 0 | . 3 25213 | 0 | ! | 0 | . 4 613303 | 0 | ! | 0 | . 5 1028188 | 0 | ! | 0 | . 6 1497620 | 0 | ! | 0 | . 7 2875276 | 0 | ! | 0 | . 8 2988645 | 0 | ! | 0 | . 9 4355567 | 0 | ! | 0 | . 10 5583438 | 0 | ! | 0 | . 11 7712754 | 0 | ! | 0 | . 12 9969569 | 0 | ! | 0 | . 13 11646457 | 0 | ! | 0 | . 14 20481393 | 0 | ! | 0 | . 15 21855996 | 0 | ! | 0 | . 16 23752827 | 0 | ! | 0 | . 17 33983238 | 0 | ! | 0 | . 18 35557493 | 0 | ! | 0 | . 19 35678765 | 0 | ! | 0 | .",
            "url": "https://dev.clockworkpanda.xyz/wikipedia/dask/neo4j/2020/12/02/daskwiki.html",
            "relUrl": "/wikipedia/dask/neo4j/2020/12/02/daskwiki.html",
            "date": " • Dec 2, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Nicolas Aspert, I work as researcher/IT manager at the Signal Processing Laboratory 2 in EPFL. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://dev.clockworkpanda.xyz/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dev.clockworkpanda.xyz/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
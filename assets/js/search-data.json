{
  
    
        "post0": {
            "title": "Pre-processing wikipedia dumps with dask (part 1)",
            "content": "Introduction . In addition to being a great source of information, Wikipedia is also a model in openness and access to its data. You can access the whole data from the dumps page and download most of the data powering Wikipedia. . I will focus here on how to process the SQL dumps and transform them to be ingested by, for instance, a Neo4j graph database. This can be done by other means, for instance using the sparkwiki tool (disclaimer: mostly written by me). Sparkwiki is great, however it requires a spark instance/cluster to work, which is not always easy to set up, even with Bigtop or elasticluster. Nowadays, the research world runs a lot of Python, and dask seems (claims ?) to be the Python-based Spark equivalent, so making an equivalent of sparkwiki in python is a good excuse to see if dask is fit for the job. . Requirements . In order to run the experiments below, you need . a good internet connection (the dumps amount for ca. 8 GB of data you need to download) | a computer with a decent amount of RAM and CPUs (I ran them on a 2x20-cores system with 128 GB of RAM, but never used it fully) and preferrably SSD storage (or a suitable cluster if you have one at hand). I will asssume a Linux-based OS, though it should work on MacOs or even Windows. | a suitable conda/pip environment with the necessary packages installed (TODO) | . SQL dumps . In this example, I will show how to process the english wikipedia database dump, the adaptation to other languages should be fairly straightforward. You can find the index of all dumps on this page, the english language SQL dumps are in the enwiki subfolder. In order to achieve speedy downloads, you can use of the mirrors close to you (I am in Switzerland, the mirror from Ume√• university is the fastest one for me, YMMV). The latest backup available (when I wrote this) is from Nov. 20th 2020, I will be using this one in my examples. . Files needed . You will need 3 files : . enwiki-20201120-page.sql.gz which contains page data (title, namespace, etc.) | enwiki-20201120-pagelinks.sql.gz which contains all links between pages within the English wikipedia edition (links across languages are stored in a different table) | enwiki-20201120-redirect.sql.gz which contains redirection information betweem pages (e.g when a page is renamed) | . You can find more detailed information about each table on the database layout page. . data_path = &#39;/data/wikipedia/20201120&#39; . Parallelism - preliminary step . Sparkwiki was able to process the compressed dumps in parallel after converting them to bz2 archives. After experimenting with dask, it turns out this simple step is not sufficient to achieve parallelism. However, dask supports reading multiple files in parallel, so an extra step is needed to convert the single-file SQL dump into multiple files. Those files can be used to re-create the database powering Wikipedia, and is just a big text file containing INSERT statements (quite a lot of them actually). . The handy Linux command split allows cutting the files into smaller chunks, without breaking SQL statements which need to be parsed later on: . zcat enwiki-20201101-redirect.sql.gz | split -l 100 --filter &#39;bzip2 &gt; $FILE.bz2&#39; - split-redirect- . This command will cut the redirect SQL dump into 100-lines bzip2-compressed (thanks to the filter parameter) chunks with a split-redirect- prefix. Check the man page of the command for more details. . We can now split the two other files. The number of lines in each chunk is hand-adjusted to have a good compromise between number of files and file size : . zcat enwiki-20201101-page.sql.gz | split -l 150 --filter &#39;bzip2 &gt; $FILE.bz2&#39; - split-page- zcat enwiki-20201101-pagelinks.sql.gz | split -l 400 --filter &#39;bzip2 &gt; $FILE.bz2&#39; - split-pagelinks- . This can take a few hours (esp. the splitting of the pagelinks file), so if you want to skip this part you can use the splits I uploaded on our S3-compatible storage hosted on SwitchEngines. . Processing SQL dumps . We are now ready to actually perform the processing. . Imports . import pandas as pd import dask import dask.bag as db import dask.dataframe as ddf import re import os . Starting the dask (local) cluster . Before running commands, we need to set up a few things for processing. NB: This is NOT the only way to do it, you can read about setting up the scheduler on a single machine, or on other types of environments such as clouds, Kubernetes, etc. One important thing to set up when running on a single computer is to have a local temporary directory: at some point when processing, dask will write data to disk. It is fairly common to have NFS-mounted directories for instance, and it is crucial for good performance to make sure data does not need to go through a network. . dask.config.set({&#39;temporary_directory&#39;: &#39;/tmp&#39;}) # make sure temp dir is local and has sufficient space. Adjust to your resources/needs ! . from dask.distributed import LocalCluster, Client . We can now start our local cluster. This will work for me, make sure you adapt to your local resources ! It is important to keep $ text{n_workers} times text{memory_limit}$ under the physical memory available (or a lower value if the computer is shared with other users and you want to be nice to others). . cluster = LocalCluster(n_workers=6, threads_per_worker=4, memory_limit=24e9) client = Client(cluster) . Helpers functions . Let us now set up a few routines and regexps (check sparkwiki for details) . re_dict = {&quot;page&quot;: &quot; (( d+),( d+),&#39;(.*?)&#39;,&#39;(.*?)&#39;,([01]),([01]),([ d .]+?),&#39;( d{14})&#39;,(.*?),( d+),( d+),(.*?),(.*?) )&quot;, &quot;redirect&quot;:&quot; (( d+),( d+),&#39;(.*?)&#39;,(.*?),(.*?) )&quot;, &quot;pagelinks&quot;:&quot; (( d+),( d+),&#39;(.*?)&#39;,( d+) )&quot;} . def filter_line(line, dump_type): return line.startswith(&#39;INSERT INTO `{}` VALUES&#39;.format(dump_type)) . def split_line(line): return line.split(&quot; VALUES &quot;)[1].strip() . def get_redirect(rec): return {&#39;from&#39;:int(rec[0]), &#39;target_ns&#39;:int(rec[1]), &#39;title&#39;:rec[2], &#39;inter_wiki&#39;:rec[3], &#39;fragment&#39;:rec[4]} . def get_page(rec): # case class WikipediaSimplePage(id:Int, title:String, isRedirect:Boolean, isNew: Boolean) return {&#39;id&#39;: int(rec[0]), &#39;namespace&#39;:int(rec[1]), &#39;title&#39;: rec[2], &#39;is_redirect&#39;:int(rec[4])==1, &#39;is_new&#39;:int(rec[5])==1} . def get_pagelinks(rec): # case class WikipediaPagelinks(from:Int, namespace:Int, title:String, fromNamespace:Int) return {&#39;from&#39;: int(rec[0]), &#39;namespace&#39;:int(rec[1]), &#39;title&#39;:rec[2], &#39;from_namespace&#39;:int(rec[3])} . Processing redirects . It may seem surprising to start with this particular table. As it is the smallest one, it is often more convenient to start and experiment processing with this one. If you make a mistake, you don&#39;t have to wait for a long time before the crash. . Read the local files into a dask bag . A dask bag is the equivalent of a Spark RDD. It supports simple operations such as filter, map, etc. . redirects_bag = db.read_text(os.path.join(data_path, &#39;splits/split-redirect-*.bz2&#39;) . Alternative: read the files from S3 bucket . It is not hosted on Amazon but on the Swiss Universities cloud SwitchEngines, hence the custom options needed below . storage_options={&#39;anon&#39;:True, &#39;client_kwargs&#39;:{&#39;endpoint_url&#39;:&#39;https://os.unil.cloud.switch.ch&#39;}} . redirects_bag = db.read_text(&#39;s3://lts2-wikipedia/enwiki/20201120/splits/split-redirect-*.bz2&#39;, storage_options=storage_options) . Tansform each record into a redirect dictionary . We can chain conveniently the operators to . filter out non INSERT statements using filter_line | split all VALUES from the INSERT using split_line and the appropriate regexp | convert to a dictionary using get_redirect | . Sparkwiki does the same operations, check the WikipediaElementParser.scala file for more details. . redirects = redirects_bag.filter(lambda x: filter_line(x, &#39;redirect&#39;)) .map(split_line) .map(lambda x:re.findall(re_dict[&#39;redirect&#39;], x)).flatten() .map(get_redirect) . Finally, the collection of small redirect dictionaries is converted to a dask DataFrame (similar to a pandas DataFrame) . redirects_df = redirects.to_dataframe() . Filter out all redirects that do not concern namespace 0 (= articles), cf. Wikipedia namespaces . redirects_df_filt = redirects_df[redirects_df[&#39;target_ns&#39;]==0] . At this point, nothing is computed yet. Setting the index and saving the resulting DataFrame to a parquet file will trigger all computations. You can monitor what is happening under the hood by opening a connection to the dask scheduler web interface which should run on port 8787. It takes less than a minute on the system I have at hand. . redirects_df_filt.set_index(&#39;from&#39;).to_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;redirect.parquet&#39;), compression=&#39;gzip&#39;, engine=&#39;fastparquet&#39;) . You can call the usual pandas DataFrame methos such as head, tailetc. However, calling them directly on redirects_df_filtwill trigger a computation. If you want to avoid this, you can reload it from disk : . redirects_df_reloaded = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;redirect.parquet&#39;)) redirects_df_reloaded.head() . target_ns title inter_wiki fragment . from . 10 0 | Computer_accessibility | &#39;&#39; | &#39;&#39; | . 13 0 | History_of_Afghanistan | &#39;&#39; | &#39;&#39; | . 14 0 | Geography_of_Afghanistan | &#39;&#39; | &#39;&#39; | . 15 0 | Demographics_of_Afghanistan | &#39;&#39; | &#39;&#39; | . 18 0 | Communications_in_Afghanistan | &#39;&#39; | &#39;&#39; | . Processing pages . Almost identical to redirects, takes longer though . Read splits from local filesystem . pages_bag = db.read_text(os.path.join(data_path, &#39;splits/split-page-*.bz2&#39;) . Alternative: read from S3 storage . pages_bag = db.read_text(&#39;s3://lts2-wikipedia/enwiki/20201120/splits/split-page-*.bz2&#39;, storage_options=storage_options) . Filter records and create dataframe . pages = pages_bag.filter(lambda x: filter_line(x, &#39;page&#39;)) .map(split_line) .map(lambda x:re.findall(re_dict[&#39;page&#39;], x)) .flatten().map(get_page) . pages_df = pages.to_dataframe() . Keep only namespace 0 pages . pages_filt_df = pages_df[pages_df[&#39;namespace&#39;]==0] . Trigger computation and save result (runs for a few minutes on my system) . pages_filt_df.set_index(&#39;id&#39;).to_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pages.parquet&#39;), compression=&#39;gzip&#39;, engine=&#39;fastparquet&#39;) . Check what we now have in the dataframe : . pages_df_reloaded = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pages.parquet&#39;)) pages_df_reloaded.head(20) . namespace title is_redirect is_new . id . 10 0 | AccessibleComputing | True | False | . 12 0 | Anarchism | False | False | . 13 0 | AfghanistanHistory | True | False | . 14 0 | AfghanistanGeography | True | False | . 15 0 | AfghanistanPeople | True | False | . 18 0 | AfghanistanCommunications | True | False | . 19 0 | AfghanistanTransportations | True | False | . 20 0 | AfghanistanMilitary | True | False | . 21 0 | AfghanistanTransnationalIssues | True | False | . 23 0 | AssistiveTechnology | True | False | . 24 0 | AmoeboidTaxa | True | False | . 25 0 | Autism | False | False | . 27 0 | AlbaniaHistory | True | False | . 29 0 | AlbaniaPeople | True | False | . 30 0 | AsWeMayThink | True | False | . 35 0 | AlbaniaGovernment | True | False | . 36 0 | AlbaniaEconomy | True | False | . 39 0 | Albedo | False | False | . 40 0 | AfroAsiaticLanguages | True | False | . 42 0 | ArtificalLanguages | True | False | . Processing pagelinks . NB: this is the biggest part to process. It seems some entries can trigger utf-8 decoding errors, hence the errors=&#39;backslashreplace&#39; addition whem reading. No index will be created to speed up computation . pagelinks_bag = db.read_text(os.path.join(data_path, &#39;splits/split-pagelinks-*.bz2&#39;), errors=&#39;backslashreplace&#39;) . Or S3-hosted splits : . pageslinks_bag = db.read_text(&#39;s3://lts2-wikipedia/enwiki/20201120/splits/split-pagelinks-*.bz2&#39;, storage_options=storage_options, errors=&#39;backslashreplace&#39;) . get pagelinks dictionaries . pagelinks = pagelinks_bag.filter(lambda x: filter_line(x, &#39;pagelinks&#39;)) .map(split_line) .map(lambda x:re.findall(re_dict[&#39;pagelinks&#39;], x)) .flatten().map(get_pagelinks) . pagelinks_df = pagelinks.to_dataframe() . Only keep links between articles (namespace == 0), discard all the others . pagelinks_filt_df = pagelinks_df[(pagelinks_df[&#39;namespace&#39;] == 0) &amp; (pagelinks_df[&#39;from_namespace&#39;] == 0)] . pagelinks_filt_df.to_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks.parquet&#39;), compression=&#39;gzip&#39;, engine=&#39;fastparquet&#39;) . pagelinks_reloaded = ddf.read_parquet(os.path.join(data_path, &#39;processed&#39;, &#39;pagelinks.parquet&#39;)) pagelinks_reloaded.head(20) . from namespace title from_namespace . 0 4748 | 0 | ! | 0 | . 1 9773 | 0 | ! | 0 | . 2 15154 | 0 | ! | 0 | . 3 25213 | 0 | ! | 0 | . 4 613303 | 0 | ! | 0 | . 5 1028188 | 0 | ! | 0 | . 6 1497620 | 0 | ! | 0 | . 7 2875276 | 0 | ! | 0 | . 8 2988645 | 0 | ! | 0 | . 9 4355567 | 0 | ! | 0 | . 10 5583438 | 0 | ! | 0 | . 11 7712754 | 0 | ! | 0 | . 12 9969569 | 0 | ! | 0 | . 13 11646457 | 0 | ! | 0 | . 14 20481393 | 0 | ! | 0 | . 15 21855996 | 0 | ! | 0 | . 16 23752827 | 0 | ! | 0 | . 17 33983238 | 0 | ! | 0 | . 18 35557493 | 0 | ! | 0 | . 19 35678765 | 0 | ! | 0 | .",
            "url": "https://dev.clockworkpanda.xyz/wikipedia/dask/neo4j/2020/12/02/daskwiki.html",
            "relUrl": "/wikipedia/dask/neo4j/2020/12/02/daskwiki.html",
            "date": " ‚Ä¢ Dec 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://dev.clockworkpanda.xyz/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://dev.clockworkpanda.xyz/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://dev.clockworkpanda.xyz/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://dev.clockworkpanda.xyz/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}